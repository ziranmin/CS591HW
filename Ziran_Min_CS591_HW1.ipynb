{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Implement Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(y_pred, y_true):\n",
    "    n = len(y_pred)\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for i in range(n):\n",
    "        if y_pred[i] == 1:\n",
    "            if y_true[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP +=1\n",
    "    \n",
    "    # edge case\n",
    "    if TP + FP == 0:\n",
    "        precision = 1\n",
    "    # general case  \n",
    "    else:\n",
    "        precision = TP/(TP + FP)\n",
    "        \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(y_pred, y_true):\n",
    "    n = len(y_pred)\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    for i in range(n):\n",
    "        if y_true[i] == 1:\n",
    "            if y_pred[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN +=1\n",
    "                \n",
    "    # edge case\n",
    "    if TP + FN == 0:\n",
    "        recall = 1\n",
    "    # general case \n",
    "    else:\n",
    "        recall = TP/(TP + FN)\n",
    "        \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fscore(y_pred, y_true):\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    \n",
    "    # edge case\n",
    "    if (precision == 0) or (recall == 0):\n",
    "        fscore = 0\n",
    "    # general case \n",
    "    else: \n",
    "        fscore = 2*precision*recall/(precision+recall)\n",
    "        \n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Majority Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_complex_feature(words):\n",
    "    # label all words as complex\n",
    "    n = len(words)\n",
    "    return n * [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_complex(data_file):\n",
    "    words, y_true = load_file(data_file)\n",
    "    \n",
    "    y_pred = all_complex_feature(words)\n",
    "    \n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    fscore = get_fscore(y_pred, y_true)\n",
    "    \n",
    "    performance = [precision, recall, fscore]\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data are the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43133333333333335, 1.0, 0.6027014438751747]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_complex(\"complex_words_training.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data are the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.418, 1.0, 0.5895627644569816]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_complex(\"complex_words_development.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Word Length Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_threshold_feature(words, threshold):\n",
    "    # label words based on length\n",
    "    result = []\n",
    "    n = len(words)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # if the lenght is shorter than threshold, then it's a simple word\n",
    "        if len(words[i]) < threshold:\n",
    "            result += [0]\n",
    "        else:\n",
    "            result += [1]\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length_threshold(training_file, development_file):\n",
    "\n",
    "    train_words, train_true = load_file(training_file)\n",
    "    dev_words, dev_true = load_file(development_file)\n",
    "\n",
    "    tran_len = len(train_words)\n",
    "    max_len = 1\n",
    "    for i in range(tran_len):\n",
    "        if len(train_words[i]) > max_len:\n",
    "            max_len = len(train_words[i])\n",
    "\n",
    "\n",
    "    best_thre = 1\n",
    "    best_f = 0\n",
    "    \n",
    "    #FInd best threshold that gives highest f1-score\n",
    "    for thre in range(1, max_len+2):\n",
    "        cur_train_pred = length_threshold_feature(train_words, thre)\n",
    "        cur_fscore = get_fscore(cur_train_pred, train_true)\n",
    "        if cur_fscore > best_f:\n",
    "            best_f = cur_fscore\n",
    "            best_thre = thre\n",
    "\n",
    "\n",
    "    train_pred = length_threshold_feature(train_words, best_thre)\n",
    "    dev_pred = length_threshold_feature(dev_words, best_thre)\n",
    "\n",
    "    tprecision = get_precision(train_pred, train_true)\n",
    "    trecall = get_recall(train_pred, train_true)\n",
    "    tfscore = get_fscore(train_pred, train_true)\n",
    "\n",
    "    dprecision = get_precision(dev_pred, dev_true)\n",
    "    drecall = get_recall(dev_pred, dev_true)\n",
    "    dfscore = get_fscore(dev_pred, dev_true)\n",
    "\n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data, and development data are the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5985877240630092, 0.8516228748068007, 0.7030303030303029],\n",
       " [0.6053511705685619, 0.8660287081339713, 0.7125984251968505])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length_threshold(\"complex_words_training.txt\", \"complex_words_development.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Word Frequency Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ngram_counts(ngram_counts_file): \n",
    "    # load words frequency counts\n",
    "    counts = defaultdict(int) \n",
    "    with gzip.open(ngram_counts_file, 'rt') as f: \n",
    "        for line in f:\n",
    "            token, count = line.strip().split('\\t') \n",
    "            if token[0].islower(): \n",
    "                counts[token] = int(count) \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = load_ngram_counts(\"ngram_counts.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_threshold_feature(words, threshold, counts):\n",
    "    # label words based on frequency\n",
    "    n = len(words)\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        if counts[words[i]] < threshold:\n",
    "            result += [1]\n",
    "        else:\n",
    "            result += [0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "\n",
    "    train_words, train_true = load_file(training_file)\n",
    "    dev_words, dev_true = load_file(development_file)\n",
    "    \n",
    "    train_len = len(train_words)\n",
    "    \n",
    "    train_counts = {}\n",
    "    freq = []\n",
    "    \n",
    "    for i in range(train_len):\n",
    "        train_counts[train_words[i]] = counts[train_words[i]]\n",
    "        \n",
    "        #if this word is complex, we put its frequency into to a list 'freq'\n",
    "        if train_true[i] == 1:\n",
    "            if counts[train_words[i]] not in freq:\n",
    "                freq += [counts[train_words[i]]]\n",
    "    \n",
    "    # add lower bound into frequency list\n",
    "    if 0 not in freq:\n",
    "        freq += [0]\n",
    "    \n",
    "    # add upper bound into frequenvy list\n",
    "    add_max = max(freq) + 1\n",
    "    freq += [add_max]\n",
    "    \n",
    "    # find best threshold that gives highest f1-score\n",
    "    best_thre = 0\n",
    "    best_f = 0\n",
    "    for thre in freq:\n",
    "        cur_train_pred = frequency_threshold_feature(train_words, thre, counts)\n",
    "        cur_fscore = get_fscore(cur_train_pred, train_true)\n",
    "        if cur_fscore > best_f:\n",
    "            best_f = cur_fscore\n",
    "            best_thre = thre\n",
    "\n",
    "\n",
    "    train_pred = frequency_threshold_feature(train_words, best_thre, counts)\n",
    "    dev_pred = frequency_threshold_feature(dev_words, best_thre, counts)\n",
    "\n",
    "    tprecision = get_precision(train_pred, train_true)\n",
    "    trecall = get_recall(train_pred, train_true)\n",
    "    tfscore = get_fscore(train_pred, train_true)\n",
    "\n",
    "    dprecision = get_precision(dev_pred, dev_true)\n",
    "    drecall = get_recall(dev_pred, dev_true)\n",
    "    dfscore = get_fscore(dev_pred, dev_true)\n",
    "\n",
    "\n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data, and development data are the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5652637187000533, 0.8199381761978362, 0.6691895301166825],\n",
       " [0.556782334384858, 0.8444976076555024, 0.6711026615969581])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_threshold(\"complex_words_training.txt\", \"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Implement Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function carries out feature engineering based on the orginal data sets into matrices as inputs for the model training and testing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_matrix(training_file, development_file, counts):\n",
    "    \n",
    "    # compute train_X and train_Y matrice based on training data\n",
    "    t_words, t_y_true = load_file(training_file)\n",
    "    t_n = len(t_words)\n",
    "    \n",
    "    t_words_len_l = []\n",
    "    t_words_freq_l = []\n",
    "    \n",
    "    # add features such as word length and word frequency\n",
    "    for i in range(t_n):\n",
    "        t_words_len_l += [len(t_words[i])]\n",
    "        t_words_freq_l += [counts[t_words[i]]]\n",
    "    \n",
    "    t_words_len_l_mean = np.mean(t_words_len_l)\n",
    "    t_words_freq_l_mean = np.mean(t_words_freq_l)\n",
    "    t_words_len_l_sd = np.std(t_words_len_l)\n",
    "    t_words_freq_l_sd = np.std(t_words_freq_l)\n",
    "    \n",
    "    # normalization\n",
    "    for i in range(t_n):\n",
    "        t_words_len_l[i] = (t_words_len_l[i] - t_words_len_l_mean)/t_words_len_l_sd\n",
    "        t_words_freq_l[i] = (t_words_freq_l[i] - t_words_freq_l_mean)/t_words_freq_l_sd\n",
    "    \n",
    "    t_train = []\n",
    "    \n",
    "    for i in range(t_n):\n",
    "        t_single_row = []\n",
    "        t_single_row += [t_words_len_l[i]]\n",
    "        t_single_row += [t_words_freq_l[i]]\n",
    "        t_train += [t_single_row]\n",
    "\n",
    "    \n",
    "    # compute test_X and test_Y matrice based on development data\n",
    "    d_words, d_y_true = load_file(development_file)\n",
    "    d_n = len(d_words)\n",
    "    \n",
    "    d_words_len_l = []\n",
    "    d_words_freq_l = []\n",
    "    \n",
    "    # add features\n",
    "    for i in range(d_n):\n",
    "        d_words_len_l += [len(d_words[i])]\n",
    "        d_words_freq_l += [counts[d_words[i]]]\n",
    "    \n",
    "    # still using the mean and standard deviation from training data to do normalization\n",
    "    for i in range(d_n):\n",
    "        d_words_len_l[i] = (d_words_len_l[i] - t_words_len_l_mean)/t_words_len_l_sd\n",
    "        d_words_freq_l[i] = (d_words_freq_l[i] - t_words_freq_l_mean)/t_words_freq_l_sd\n",
    "    \n",
    "    d_train = []\n",
    "    \n",
    "    for i in range(d_n):\n",
    "        d_single_row = []\n",
    "        d_single_row += [d_words_len_l[i]]\n",
    "        d_single_row += [d_words_freq_l[i]]\n",
    "        d_train += [d_single_row]\n",
    "    \n",
    "    \n",
    "    return np.array(t_train), np.array(t_y_true), np.array(d_train), np.array(d_y_true), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Naive Bayes Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def naive_bayes(training_file, development_file, counts):\n",
    "\n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix(training_file, development_file, counts)      \n",
    "        \n",
    "        clf = GaussianNB()\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "\n",
    "        return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.4918351477449456, 0.9775888717156105, 0.6544231764097258),\n",
       " (0.4700352526439483, 0.9569377990430622, 0.6304176516942475))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " def logistic_regression(training_file, development_file, counts):\n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix(training_file, development_file, counts)      \n",
    "        \n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "\n",
    "        return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7206751054852321, 0.6599690880989181, 0.6889874949576441),\n",
       " (0.7229219143576826, 0.6866028708133971, 0.7042944785276073))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Implement Your Own Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import count_syllables function to add feature \"number of syllables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "## Counts number of syllables (from eayd.in/?p=232)\n",
    "def count_syllables(word): \n",
    "    word = word.lower() \n",
    "    # exception_add are words that need extra syllables\n",
    "    # exception_del are words that need less syllables\n",
    "    exception_add = ['serious','crucial']\n",
    "    exception_del = ['fortunately','unfortunately'] \n",
    "    co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
    "    co_two = ['coapt','coed','coinci']\n",
    "    pre_one = ['preach']\n",
    " \n",
    "    syls = 0 #added syllable number\n",
    "    disc = 0 #discarded syllable number\n",
    " \n",
    "    #1) if letters < 3 : return 1\n",
    "    if len(word) <= 3 :\n",
    "        syls = 1\n",
    "        return syls\n",
    " \n",
    "    #2) if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
    "    # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.) \n",
    "    if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
    "        doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "        if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
    "            if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
    "                pass\n",
    "            else :\n",
    "                disc+=1\n",
    " \n",
    "    #3) discard trailing \"e\", except where ending is \"le\"   \n",
    "    le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while'] \n",
    "    if word[-1:] == \"e\" :\n",
    "        if word[-2:] == \"le\" and word not in le_except :\n",
    "            pass \n",
    "        else :\n",
    "            disc+=1\n",
    " \n",
    "    #4) check if consecutive vowels exists, triplets or pairs, count them as one. \n",
    "    doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "    tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
    "    disc+=doubleAndtripple + tripple\n",
    " \n",
    "    #5) count remaining vowels in word.\n",
    "    numVowels = len(re.findall(r'[eaoui]',word))\n",
    " \n",
    "    #6) add one if starts with \"mc\"\n",
    "    if word[:2] == \"mc\" :\n",
    "        syls+=1\n",
    " \n",
    "    #7) add one if ends with \"y\" but is not surrouned by vowel\n",
    "    if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
    "        syls +=1\n",
    " \n",
    "    #8) add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
    "    for i,j in enumerate(word) :\n",
    "        if j == \"y\" :\n",
    "            if (i != 0) and (i != len(word)-1) :\n",
    "                if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
    "                    syls+=1\n",
    " \n",
    "    #9) if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one. \n",
    "    if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
    "        syls+=1 \n",
    "    if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
    "        syls+=1\n",
    " \n",
    "    #10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
    "    if word[-3:] == \"ian\" : \n",
    "    #and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
    "        if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    " \n",
    "    #11) if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly. \n",
    "    if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
    "        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
    "            syls+=1\n",
    "        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    " \n",
    "    #12) if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly. \n",
    "    if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
    "        if word[:6] in pre_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    " \n",
    "    #13) check for \"-n't\" and cross match with dictionary to add syllable.\n",
    "    negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"] \n",
    "    if word[-3:] == \"n't\" :\n",
    "        if word in negative :\n",
    "            syls+=1\n",
    "        else :\n",
    "            pass  \n",
    " \n",
    "    #14) Handling the exceptional words. \n",
    "    if word in exception_del :\n",
    "        disc+=1 \n",
    "    if word in exception_add :\n",
    "        syls+=1    \n",
    " \n",
    "    # calculate the output\n",
    "    return numVowels - disc + syls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Wordnet and build function to add features \"number of synoyms\" and \"number of antonyms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syn_ant(word):\n",
    "    synonyms = [] \n",
    "    antonyms = [] \n",
    "  \n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonyms.append(l.name()) \n",
    "            if l.antonyms(): \n",
    "                antonyms.append(l.antonyms()[0].name()) \n",
    "                \n",
    "    return len(set(synonyms)), len(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add sentence complexituy features, \"length of the sentence\", \"average word length\", and \"average word frequency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update load file function to load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_new(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    sentence = [] \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "                \n",
    "                sentence_str = line_split[3].split()\n",
    "                sentence_list = [i for i in sentence_str if i not in \",.!?:;@#$%^&*()``~''--+=|\\/<>\"]   \n",
    "                sentence.append(sentence_list)\n",
    "                \n",
    "            i += 1\n",
    "    return words, labels, sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_words_number(sentence):\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_words_avg_len(sentence):\n",
    "    n = len(sentence)\n",
    "    total_char = 0\n",
    "    for i in range(n):\n",
    "        total_char += len(sentence[i])\n",
    "    return total_char/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_avg_word_freq(sentence, counts):\n",
    "    n = len(sentence)\n",
    "    total_freq = 0\n",
    "    for i in range(n):\n",
    "        total_freq += counts[sentence[i]]\n",
    "    return total_freq/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I have features \"word lenght\", \"word frequency\", \"number of syllables\", \"number of synoyms\", and \"number of antonyms\". I will carry out the following feature engineering function to make matrices for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_matrix_mine(training_file, development_file, counts):\n",
    "    \n",
    "    # compute train_X and train_Y matrice based on training data\n",
    "    words, y_true, sentences = load_file_new(training_file)\n",
    "    \n",
    "    n = len(words)\n",
    "    words_len_l = []\n",
    "    words_freq_l = []\n",
    "    words_syllable_l = []\n",
    "    words_synonyms_count_l = []\n",
    "    words_antonyms_count_l = []\n",
    "    sent_len_l = []\n",
    "    sent_words_avg_len_l = []\n",
    "    sent_words_avg_freq_l = []\n",
    "    \n",
    "    # add features \n",
    "    for i in range(n):\n",
    "        words_len_l += [len(words[i])]\n",
    "        words_freq_l += [counts[words[i]]] \n",
    "        words_syllable_l += [count_syllables(words[i])]\n",
    "        words_synonyms_count_l += [count_syn_ant(words[i])[0]]\n",
    "        words_antonyms_count_l += [count_syn_ant(words[i])[1]]\n",
    "        sent_len_l += [sentence_words_number(sentences[i])]\n",
    "        sent_words_avg_len_l += [sentence_words_avg_len(sentences[i])]\n",
    "        sent_words_avg_freq_l += [sentence_avg_word_freq(sentences[i], counts)]\n",
    "    \n",
    "    # compute means and standard deviation \n",
    "    words_len_l_mean = np.mean(words_len_l)\n",
    "    words_freq_l_mean = np.mean(words_freq_l)\n",
    "    words_syllable_l_mean = np.mean(words_syllable_l)\n",
    "    words_synonyms_count_l_mean = np.mean(words_synonyms_count_l)\n",
    "    words_antonyms_count_l_mean = np.mean(words_antonyms_count_l)   \n",
    "    sent_len_l_mean = np.mean(sent_len_l)\n",
    "    sent_words_avg_len_l_mean = np.mean(sent_words_avg_len_l)\n",
    "    sent_words_avg_freq_l_mean = np.mean(sent_words_avg_freq_l)\n",
    "\n",
    "    words_len_l_sd = np.std(words_len_l)\n",
    "    words_freq_l_sd = np.std(words_freq_l)\n",
    "    words_syllable_l_sd = np.std(words_syllable_l)\n",
    "    words_synonyms_count_l_sd = np.std(words_synonyms_count_l)\n",
    "    words_antonyms_count_l_sd = np.std(words_antonyms_count_l)\n",
    "    sent_len_l_sd = np.std(sent_len_l)\n",
    "    sent_words_avg_len_l_sd = np.std(sent_words_avg_len_l)\n",
    "    sent_words_avg_freq_l_sd = np.std(sent_words_avg_freq_l)\n",
    "                                       \n",
    "    # normalization\n",
    "    for i in range(n):\n",
    "        words_len_l[i] = (words_len_l[i] - words_len_l_mean)/words_len_l_sd\n",
    "        words_freq_l[i] = (words_freq_l[i] - words_freq_l_mean)/words_freq_l_sd\n",
    "        words_syllable_l[i] = (words_syllable_l[i] - words_syllable_l_mean)/words_syllable_l_sd\n",
    "        words_synonyms_count_l[i] = (words_synonyms_count_l[i] - words_synonyms_count_l_mean)/words_synonyms_count_l_sd\n",
    "        words_antonyms_count_l[i] = (words_antonyms_count_l[i] - words_antonyms_count_l_mean)/words_antonyms_count_l_sd          \n",
    "        sent_len_l[i] = (sent_len_l[i] - sent_len_l_mean)/sent_len_l_sd\n",
    "        sent_words_avg_len_l[i] = (sent_words_avg_len_l[i] - sent_words_avg_len_l_mean)/sent_words_avg_len_l_sd\n",
    "        sent_words_avg_freq_l[i] = (sent_words_avg_freq_l[i] - sent_words_avg_freq_l_mean)/sent_words_avg_freq_l_sd\n",
    "                                                          \n",
    "    train = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        single_row = []\n",
    "        single_row += [words_len_l[i]]\n",
    "        single_row += [words_freq_l[i]]\n",
    "        single_row += [words_syllable_l[i]]\n",
    "        single_row += [words_synonyms_count_l[i]]\n",
    "        single_row += [words_antonyms_count_l[i]]                 \n",
    "        single_row += [sent_len_l[i]]\n",
    "        single_row += [sent_words_avg_len_l[i]]\n",
    "        single_row += [sent_words_avg_freq_l[i]]\n",
    "        train += [single_row]\n",
    "                             \n",
    "            \n",
    "    # compute test_X and test_Y matrice based on development data\n",
    "    d_words, d_y_true, d_sentences = load_file_new(development_file)\n",
    "    \n",
    "    d_n = len(d_words)\n",
    "    d_words_len_l = []\n",
    "    d_words_freq_l = []\n",
    "    d_words_syllable_l = []\n",
    "    d_words_synonyms_count_l = []\n",
    "    d_words_antonyms_count_l = []\n",
    "    d_sent_len_l = []\n",
    "    d_sent_words_avg_len_l = []\n",
    "    d_sent_words_avg_freq_l = []\n",
    "    \n",
    "    # add features\n",
    "    for i in range(d_n):\n",
    "        d_words_len_l += [len(d_words[i])]\n",
    "        d_words_freq_l += [counts[d_words[i]]] \n",
    "        d_words_syllable_l += [count_syllables(d_words[i])]\n",
    "        d_words_synonyms_count_l += [count_syn_ant(d_words[i])[0]]\n",
    "        d_words_antonyms_count_l += [count_syn_ant(d_words[i])[1]]\n",
    "        d_sent_len_l += [sentence_words_number(sentences[i])]\n",
    "        d_sent_words_avg_len_l += [sentence_words_avg_len(sentences[i])]\n",
    "        d_sent_words_avg_freq_l += [sentence_avg_word_freq(sentences[i], counts)]\n",
    "    \n",
    "    # still using the mean and standard deviation from training data to do normalization\n",
    "    for i in range(d_n):\n",
    "        d_words_len_l[i] = (d_words_len_l[i] - words_len_l_mean)/words_len_l_sd\n",
    "        d_words_freq_l[i] = (d_words_freq_l[i] - words_freq_l_mean)/words_freq_l_sd\n",
    "        d_words_syllable_l[i] = (d_words_syllable_l[i] - words_syllable_l_mean)/words_syllable_l_sd\n",
    "        d_words_synonyms_count_l[i] = (d_words_synonyms_count_l[i] - words_synonyms_count_l_mean)/words_synonyms_count_l_sd\n",
    "        d_words_antonyms_count_l[i] = (d_words_antonyms_count_l[i] - words_antonyms_count_l_mean)/words_antonyms_count_l_sd          \n",
    "        d_sent_len_l[i] = (d_sent_len_l[i] - sent_len_l_mean)/sent_len_l_sd\n",
    "        d_sent_words_avg_len_l[i] = (d_sent_words_avg_len_l[i] - sent_words_avg_len_l_mean)/sent_words_avg_len_l_sd\n",
    "        d_sent_words_avg_freq_l[i] = (d_sent_words_avg_freq_l[i] - sent_words_avg_freq_l_mean)/sent_words_avg_freq_l_sd\n",
    "    \n",
    "                            \n",
    "    d_train = []\n",
    "    \n",
    "    for i in range(d_n):\n",
    "        d_single_row = []\n",
    "        d_single_row += [d_words_len_l[i]]\n",
    "        d_single_row += [d_words_freq_l[i]]\n",
    "        d_single_row += [d_words_syllable_l[i]]\n",
    "        d_single_row += [d_words_synonyms_count_l[i]]\n",
    "        d_single_row += [d_words_antonyms_count_l[i]]  \n",
    "        d_single_row += [d_sent_len_l[i]]\n",
    "        d_single_row += [d_sent_words_avg_len_l[i]]\n",
    "        d_single_row += [d_sent_words_avg_freq_l[i]]\n",
    "        d_train += [d_single_row]\n",
    "\n",
    "                              \n",
    "    return np.array(train), np.array(y_true), np.array(d_train), np.array(d_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different classificaiton models and compare scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    " def naive_bayes_new(training_file, development_file, counts):  \n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix_mine(training_file, development_file, counts)      \n",
    "        \n",
    "        clf = GaussianNB()\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "        \n",
    "        return training_performance, development_performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.5347826086956522, 0.9505409582689336, 0.6844741235392322),\n",
       " (0.524, 0.9401913875598086, 0.672945205479452))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_new(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " def logistic_regression_new(training_file, development_file, counts):\n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix_mine(training_file, development_file, counts)      \n",
    "        \n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "\n",
    "        return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7244986922406277, 0.6421947449768161, 0.6808684965178206),\n",
       " (0.7313829787234043, 0.6578947368421053, 0.6926952141057935))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_new(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " def random_forest(training_file, development_file, counts):\n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix_mine(training_file, development_file, counts)      \n",
    "        \n",
    "        # we have five features\n",
    "        clf = RandomForestClassifier(max_depth = 8)\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "\n",
    "        return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7891304347826087, 0.8415765069551777, 0.8145100972326104),\n",
       " (0.6962025316455697, 0.7894736842105263, 0.7399103139013453))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4)Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    " def decision_tree(training_file, development_file, counts):\n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix_mine(training_file, development_file, counts)      \n",
    "        \n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "\n",
    "        return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.0, 1.0, 1.0), (0.6333333333333333, 0.6363636363636364, 0.6348448687350836))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that decision tree overfits the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    " def knn(training_file, development_file, counts):\n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix_mine(training_file, development_file, counts)      \n",
    "        \n",
    "        clf = KNeighborsClassifier()\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "\n",
    "        return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7596463022508039, 0.7302936630602782, 0.7446808510638298),\n",
       " (0.6339712918660287, 0.6339712918660287, 0.6339712918660287))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    " def support_vector_machine(training_file, development_file, counts):\n",
    "        train_x, train_y_true, dev_x, dev_y_true = make_data_matrix_mine(training_file, development_file, counts)      \n",
    "        \n",
    "        clf = svm.SVC()\n",
    "        clf.fit(train_x, train_y_true)\n",
    "        \n",
    "        train_y_pred = clf.predict(train_x)\n",
    "        train_y_pred_l = train_y_pred.tolist()\n",
    "        \n",
    "        tprecision = get_precision(train_y_pred_l, train_y_true)\n",
    "        trecall = get_recall(train_y_pred_l, train_y_true)\n",
    "        tfscore = get_fscore(train_y_pred_l, train_y_true)\n",
    "          \n",
    "        dev_y_pred = clf.predict(dev_x)\n",
    "        dev_y_pred_l = dev_y_pred.tolist()\n",
    "        \n",
    "        dprecision = get_precision(dev_y_pred_l, dev_y_true)\n",
    "        drecall = get_recall(dev_y_pred_l, dev_y_true)\n",
    "        dfscore = get_fscore(dev_y_pred_l, dev_y_true)\n",
    "        \n",
    "        \n",
    "        training_performance = (tprecision, trecall, tfscore)\n",
    "        development_performance = (dprecision, drecall, dfscore)\n",
    "\n",
    "        return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The precision, recall, and f-score on training data is in the first line of the following output. \n",
    "#### The precision, recall, and f-score on development data is in the second line of the following output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7254575707154742, 0.6738794435857806, 0.6987179487179487),\n",
       " (0.7186700767263428, 0.6722488038277512, 0.6946847960444994))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_vector_machine(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In all, I find that Random Forest Model gives me the best result based on f1-scores. So I will combine the training data and development data as one to train the Random Forest Model, and make it to predict the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the unlabeled data file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_test(data_file):\n",
    "    words = []\n",
    "    sentence = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                sentence_str = line_split[1].split()\n",
    "                sentence_list = [i for i in sentence_str if i not in \",.!?:;@#$%^&*()``~''--+=|\\/<>\"]   \n",
    "                sentence.append(sentence_list)\n",
    "            i += 1\n",
    "    return words, sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do feature engineering for the predicting dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_matrix_for_test(training_file, test_file, counts):\n",
    "    \n",
    "    # Load training data first to get the means and standard deviations of each feature\n",
    "    words, y_true, sentences = load_file_new(training_file)\n",
    "    n = len(words)\n",
    "    words_len_l = []\n",
    "    words_freq_l = []\n",
    "    words_syllable_l = []\n",
    "    words_synonyms_count_l = []\n",
    "    words_antonyms_count_l = []\n",
    "    sent_len_l = []\n",
    "    sent_words_avg_len_l = []\n",
    "    sent_words_avg_freq_l = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        words_len_l += [len(words[i])]\n",
    "        words_freq_l += [counts[words[i]]] \n",
    "        words_syllable_l += [count_syllables(words[i])]\n",
    "        words_synonyms_count_l += [count_syn_ant(words[i])[0]]\n",
    "        words_antonyms_count_l += [count_syn_ant(words[i])[1]]\n",
    "        sent_len_l += [sentence_words_number(sentences[i])]\n",
    "        sent_words_avg_len_l += [sentence_words_avg_len(sentences[i])]\n",
    "        sent_words_avg_freq_l += [sentence_avg_word_freq(sentences[i], counts)]\n",
    "    \n",
    "    # get the means and standard deviations of each feature\n",
    "    words_len_l_mean = np.mean(words_len_l)\n",
    "    words_freq_l_mean = np.mean(words_freq_l)\n",
    "    words_syllable_l_mean = np.mean(words_syllable_l)\n",
    "    words_synonyms_count_l_mean = np.mean(words_synonyms_count_l)\n",
    "    words_antonyms_count_l_mean = np.mean(words_antonyms_count_l)       \n",
    "    sent_len_l_mean = np.mean(sent_len_l)\n",
    "    sent_words_avg_len_l_mean = np.mean(sent_words_avg_len_l)\n",
    "    sent_words_avg_freq_l_mean = np.mean(sent_words_avg_freq_l)\n",
    "    \n",
    "    words_len_l_sd = np.std(words_len_l)\n",
    "    words_freq_l_sd = np.std(words_freq_l)\n",
    "    words_syllable_l_sd = np.std(words_syllable_l)\n",
    "    words_synonyms_count_l_sd = np.std(words_synonyms_count_l)\n",
    "    words_antonyms_count_l_sd = np.std(words_antonyms_count_l)\n",
    "    sent_len_l_sd = np.std(sent_len_l)\n",
    "    sent_words_avg_len_l_sd = np.std(sent_words_avg_len_l)\n",
    "    sent_words_avg_freq_l_sd = np.std(sent_words_avg_freq_l)\n",
    "    \n",
    "    \n",
    "    # load unlabeled tesing data            \n",
    "    t_words,  t_sentences = load_file_test(test_file)\n",
    "    t_n = len(t_words)\n",
    "    t_words_len_l = []\n",
    "    t_words_freq_l = []\n",
    "    t_words_syllable_l = []\n",
    "    t_words_synonyms_count_l = []\n",
    "    t_words_antonyms_count_l = []\n",
    "    t_sent_len_l = []\n",
    "    t_sent_words_avg_len_l = []\n",
    "    t_sent_words_avg_freq_l = []\n",
    "    \n",
    "    # add features\n",
    "    for i in range(t_n):\n",
    "        t_words_len_l += [len(t_words[i])]\n",
    "        t_words_freq_l += [counts[t_words[i]]] \n",
    "        t_words_syllable_l += [count_syllables(t_words[i])]\n",
    "        t_words_synonyms_count_l += [count_syn_ant(t_words[i])[0]]\n",
    "        t_words_antonyms_count_l += [count_syn_ant(t_words[i])[1]]\n",
    "        t_sent_len_l += [sentence_words_number(t_sentences[i])]\n",
    "        t_sent_words_avg_len_l += [sentence_words_avg_len(t_sentences[i])]\n",
    "        t_sent_words_avg_freq_l += [sentence_avg_word_freq(t_sentences[i], counts)]\n",
    "    \n",
    "    # normalization\n",
    "    for i in range(t_n):\n",
    "        t_words_len_l[i] = (t_words_len_l[i] - words_len_l_mean)/words_len_l_sd\n",
    "        t_words_freq_l[i] = (t_words_freq_l[i] - words_freq_l_mean)/words_freq_l_sd\n",
    "        t_words_syllable_l[i] = (t_words_syllable_l[i] - words_syllable_l_mean)/words_syllable_l_sd\n",
    "        t_words_synonyms_count_l[i] = (t_words_synonyms_count_l[i] - words_synonyms_count_l_mean)/words_synonyms_count_l_sd\n",
    "        t_words_antonyms_count_l[i] = (t_words_antonyms_count_l[i] - words_antonyms_count_l_mean)/words_antonyms_count_l_sd                    \n",
    "        t_sent_len_l[i] = (t_sent_len_l[i] - sent_len_l_mean)/sent_len_l_sd\n",
    "        t_sent_words_avg_len_l[i] = (t_sent_words_avg_len_l[i] - sent_words_avg_len_l_mean)/sent_words_avg_len_l_sd\n",
    "        t_sent_words_avg_freq_l[i] = (t_sent_words_avg_freq_l[i] - sent_words_avg_freq_l_mean)/sent_words_avg_freq_l_sd\n",
    "    \n",
    "                            \n",
    "    t_train = []\n",
    "    \n",
    "    for i in range(t_n):\n",
    "        t_single_row = []\n",
    "        t_single_row += [t_words_len_l[i]]\n",
    "        t_single_row += [t_words_freq_l[i]]\n",
    "        t_single_row += [t_words_syllable_l[i]]\n",
    "        t_single_row += [t_words_synonyms_count_l[i]]\n",
    "        t_single_row += [t_words_antonyms_count_l[i]]                    \n",
    "        t_single_row += [t_sent_len_l[i]]\n",
    "        t_single_row += [t_sent_words_avg_len_l[i]]\n",
    "        t_single_row += [t_sent_words_avg_freq_l[i]]\n",
    "        t_train += [t_single_row]\n",
    "    \n",
    "    return np.array(t_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine training and development datasets, train random forest model on it, and make prediction on unlabeled testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y_true, dev_x, dev_y_true = make_data_matrix_mine(\"complex_words_training.txt\",\"complex_words_development.txt\", counts)      \n",
    "x_total = np.concatenate((train_x, dev_x), axis=0)\n",
    "y_true_total = np.concatenate((train_y_true, dev_y_true), axis=0)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=8)\n",
    "clf.fit(x_total, y_true_total)\n",
    "\n",
    "y_pred =  clf.predict(x_total)\n",
    "precision = get_precision(y_pred, y_true_total)\n",
    "recall = get_recall(y_pred, y_true_total)\n",
    "fscore = get_fscore(y_pred, y_true_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The precision, recall and f1-score based on model trained and test on combined model are the following output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8006737787759686, 0.8329439252336449, 0.8164901231033497)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction on unlabeld data and save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = make_data_matrix_for_test(\"complex_words_training.txt\", \"complex_words_test_unlabeled.txt\", counts)\n",
    "final_result = clf.predict(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Ziran_Min_Predicted Results.txt\",final_result,fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
